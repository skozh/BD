{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd02368ea13b64a04b371283ef5424a3f39696477dc8697c632bb26cbef3b68e54b",
   "display_name": "Python 3.7.10 64-bit ('sparkafka': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "2368ea13b64a04b371283ef5424a3f39696477dc8697c632bb26cbef3b68e54b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from apscheduler.schedulers.blocking import BlockingScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .appName(\"Profile Stream Consumer Log\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"post_id\", IntegerType()), \\\n",
    "            StructField(\"sex\", IntegerType()), StructField(\"age\", IntegerType()), \\\n",
    "            StructField(\"totalwords\", IntegerType()), StructField(\"totalfiltwords\", IntegerType())])\n",
    "\n",
    "\n",
    "def timed_job():\n",
    "    print(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "    df = spark.read.csv('output/*.csv')\n",
    "    if (df.count() > 0):\n",
    "        df = df.withColumn('json', from_json('_c0', schema)).select('json')\n",
    "        print(\"Total Number of Records = \", df.count())\n",
    "        print(\"Total Posts by Males = \", df.filter(\"json.sex==1\").count())\n",
    "        print(\"Total Posts by Females = \", df.filter(\"json.sex==2\").count())\n",
    "        print(\"Total Posts by age < 18 = \", df.filter(\"json.age<18\").count())\n",
    "        print(\"Total Posts by age >= 18 & < 27 = \", df.filter(\"json.age>=18 and json.age<27\").count())\n",
    "        print(\"Total Posts by age >= 27 & < 40 = \", df.filter(\"json.age>=27 and json.age<40\").count())\n",
    "        print(\"Total Posts by age >= 40 & < 60 = \", df.filter(\"json.age>=40 and json.age<60\").count())\n",
    "        print(\"Total Posts by age >= 60 = \", df.filter(\"json.age>=60\").count())\n",
    "        print(\"Total Number of words = \", df.agg(sum(\"json.totalwords\")).collect()[0][0])\n",
    "        print(\"Total Number of words (except stopwords) = \", df.agg(sum(\"json.totalfiltwords\")).collect()[0][0])\n",
    "    else:\n",
    "        print(\"No records received!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = BlockingScheduler()\n",
    "timed_job()\n",
    "scheduler.add_job(timed_job, 'interval', minutes=10)\n",
    "scheduler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ]
}