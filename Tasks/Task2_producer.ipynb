{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd02368ea13b64a04b371283ef5424a3f39696477dc8697c632bb26cbef3b68e54b",
   "display_name": "Python 3.7.10 64-bit ('sparkafka': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "2368ea13b64a04b371283ef5424a3f39696477dc8697c632bb26cbef3b68e54b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import json\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "KAFKA_HOSTS = 'localhost:9092'\n",
    "KAFKA_VERSION = (0, 10, 2)\n",
    "topics = 'posts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "    .appName(\"Profile Stream Producer\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Message\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_HOSTS, api_version=KAFKA_VERSION, value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "producer.send(\"test\", \"Hello World!\")\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "profile_df = spark.read.json(\"shared_data/bigdata20/followers_info.json/*.json\")\n",
    "posts_df = spark.read.json(\"shared_data/bigdata20/followers_posts_api_final.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add age and sex of the post owner to the posts dataset\n",
    "\n",
    "find_year = F.udf(lambda y: y[-4:] if y else 0, StringType())\n",
    "find_age = F.udf(lambda y: (np.random.randint(15,80)) if y==0 else (datetime.datetime.now().year - y), IntegerType())\n",
    "\n",
    "\n",
    "profile_df = profile_df.withColumn(\"year\",\\\n",
    "                        F.when(F.col('bdate').isNull() | (F.length(F.col('bdate'))<7), '0')\\\n",
    "                        .otherwise(find_year(F.col('bdate'))))\n",
    "profile_df = profile_df.withColumn('age', find_age(profile_df[\"year\"].cast(IntegerType())))\\\n",
    "                        .select(F.col('id').alias('owner_id'), F.col('sex'), F.col('age'))\n",
    "\n",
    "posts_df = posts_df.join(profile_df, \"owner_id\", \"left\")\n",
    "data_df = posts_df.withColumnRenamed('id', 'post_id').na\\\n",
    "                    .drop(subset=['age', 'sex', 'post_id', 'owner_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "\n",
    "stopwords = spark.read.text('data/stopwords.txt')\n",
    "val = stopwords.select('value').collect()\n",
    "wordlist = [ele['value'] for ele in val]\n",
    "x = \",\".join(wordlist)\n",
    "\n",
    "text_df = data_df.select('post_id', 'text').withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"\\n\", \"\"))\\\n",
    "                    .withColumn(\"unfilteredText\", F.split(\"text\", \" \"))\n",
    "text_df = text_df.withColumn(\"filter_col\", F.lit(x))\\\n",
    "                    .withColumn(\"filter_col\", F.split(\"filter_col\", \",\"))\\\n",
    "                    .withColumn(\"filteredText\", F.array_except(\"unfilteredText\", \"filter_col\"))\\\n",
    "                    .na.drop(subset=['filteredText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data\n",
    "\n",
    "data_df = data_df.join(text_df.select('post_id','filteredText', 'unfilteredText'), on='post_id', how='left')\\\n",
    "                .withColumn('totalwords', F.size('unfilteredText'))\\\n",
    "                .withColumn('totalfiltwords', F.size('filteredText'))\\\n",
    "                .select(\"post_id\", \"sex\", \"age\", \"totalwords\", \"totalfiltwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send message\n",
    "\n",
    "def sendMessage(row):\n",
    "    producer = KafkaProducer(bootstrap_servers=KAFKA_HOSTS, api_version=KAFKA_VERSION, value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "    producer.send(topics, row.asDict())\n",
    "    producer.flush()\n",
    "\n",
    "data_df.rdd.foreach(sendMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}