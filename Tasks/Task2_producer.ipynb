{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd02368ea13b64a04b371283ef5424a3f39696477dc8697c632bb26cbef3b68e54b",
   "display_name": "Python 3.7.10 64-bit ('sparkafka': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "2368ea13b64a04b371283ef5424a3f39696477dc8697c632bb26cbef3b68e54b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "KAFKA_HOSTS = 'localhost:9092'\n",
    "KAFKA_VERSION = (0, 10, 2)\n",
    "TOPIC = \"profile1\"\n",
    "dataDirectory = \"shared_data/bigdata20/followers_info.json/*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .appName(\"Profile Stream Producer\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "profile_df = spark.read.json(\"shared_data/bigdata20/followers_info.json/*.json\")\n",
    "posts_df = spark.read.json(\"shared_data/bigdata20/followers_posts_api_final.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add age and sex of the owner_id to the post_info\n",
    "\n",
    "find_year = F.udf(lambda y: y[-4:] if y else 0, StringType())\n",
    "find_age = F.udf(lambda y: (np.random.randint(18,80)) if y==0 else (datetime.datetime.now().year - y), IntegerType())\n",
    "\n",
    "\n",
    "profile_df = profile_df.withColumn(\"year\",\\\n",
    "                        F.when(F.col('bdate').isNull() | (F.length(F.col('bdate'))<7), '0')\\\n",
    "                        .otherwise(find_year(F.col('bdate'))))\n",
    "profile_df = profile_df.withColumn('age', find_age(profile_df[\"year\"].cast(IntegerType())))\\\n",
    "                        .select(F.col('id').alias('owner_id'), F.col('sex'), F.col('age'))\n",
    "\n",
    "posts_df = posts_df.join(profile_df, \"owner_id\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = posts_df.select(F.col(\"id\").alias(\"post_id\"), \\\n",
    "                    F.col(\"owner_id\"), F.col(\"age\"), F.col(\"sex\")).na.drop()\n",
    "\n",
    "ds = data_df\\\n",
    "    .select(F.to_json(F.struct([F.col(c).alias(c) for c in data_df.columns])).cast(\"string\").alias('value'))\\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_HOSTS) \\\n",
    "    .option(\"topic\", TOPIC) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ]
}